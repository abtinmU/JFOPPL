{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Note: To speed up iteration and debugging, this notebook defines the custom modules inline instead of importing them from the external .jl files. Once the code stabilizes, I will switch back to proper `include(\"Module.jl\")`-style imports."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JWwEtIHCK_F4"
      },
      "source": [
        "# Packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NvGi5Nvy1vxl",
        "outputId": "f180d040-eabe-4d71-add2-12314c708908"
      },
      "outputs": [],
      "source": [
        "using Pkg\n",
        "\n",
        "Pkg.activate(mktempdir())\n",
        "\n",
        "Pkg.add([\n",
        "    \"Distributions\",\n",
        "    \"StatsFuns\",\n",
        "    \"DataStructures\",\n",
        "    \"JSON\",\n",
        "    \"StatsBase\",\n",
        "    \"DataFrames\",\n",
        "    \"Plots\",\n",
        "    \"Zygote\",\n",
        "    \"Optimisers\",\n",
        "    \"SpecialFunctions\"\n",
        "])\n",
        "Pkg.precompile()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wsh-7i5eAhRm"
      },
      "source": [
        " # Utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u3MJA2KvAjO1",
        "outputId": "a8a44611-48f8-4f86-ce44-cb2c8dbb4e8d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Main.SampleHandling"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "module SampleHandling\n",
        "\n",
        "using Random\n",
        "using Statistics\n",
        "using LinearAlgebra\n",
        "using DataStructures: OrderedDict\n",
        "\n",
        "# Flatten sample function\n",
        "function flatten_sample(sample)\n",
        "    if isa(sample, AbstractVector{<:AbstractVector})\n",
        "        flat_sample = vcat([vec(element) for element in sample]...)\n",
        "    else\n",
        "        flat_sample = sample\n",
        "    end\n",
        "    return flat_sample\n",
        "end\n",
        "\n",
        "# Create unique list function\n",
        "function create_unique_list(list_with_duplicates)\n",
        "    return collect(OrderedDict(zip(list_with_duplicates, 1:length(list_with_duplicates))))\n",
        "end\n",
        "\n",
        "# Burn chain function\n",
        "function burn_chain(samples, weights, burn_frac=nothing)\n",
        "    if burn_frac !== nothing\n",
        "        n = length(samples)\n",
        "        nburn = Int(burn_frac * n)\n",
        "        burned_samples = samples[nburn+1:end]\n",
        "        burned_weights = weights[nburn+1:end]\n",
        "        return burned_samples, burned_weights\n",
        "    else\n",
        "        return samples, weights\n",
        "    end\n",
        "end\n",
        "\n",
        "end # module SampleHandling\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aZ6jGbBmAjMu",
        "outputId": "4e9b7482-8545-4cd4-bced-d2eccf07641f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Main.MathOps"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "module MathOps\n",
        "\n",
        "using Random\n",
        "using Statistics\n",
        "using LinearAlgebra\n",
        "using StatsFuns: logsumexp\n",
        "\n",
        "# Softplus function\n",
        "function softplus(x, beta=1.0, threshold=20.0)\n",
        "    s = ifelse(x <= threshold, log(exp(beta * x) + 1.0) / beta, x)\n",
        "    return s\n",
        "end\n",
        "\n",
        "# Inverse Softplus function\n",
        "function inverse_softplus(s, beta=1.0, threshold=20.0)\n",
        "    x = ifelse(s <= threshold, log(exp(beta * s) - 1.0) / beta, s)\n",
        "    return x\n",
        "end\n",
        "\n",
        "# Covariance function\n",
        "function covariance(x, y)\n",
        "    return mean(x .* y, dims=1) - mean(x, dims=1) .* mean(y, dims=1)\n",
        "end\n",
        "\n",
        "end # module MathOps\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hJSdDyoxAjKu",
        "outputId": "4dfb507b-2ad4-492f-d2a6-d297b735ac2b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Main.Plotting"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "module Plotting\n",
        "\n",
        "using Random\n",
        "using Statistics\n",
        "using StatsBase\n",
        "using DataFrames\n",
        "ENV[\"GKSwstype\"] = \"100\"\n",
        "using Plots\n",
        "\n",
        "# Function to create a histogram\n",
        "function create_histogram(data, column, title)\n",
        "    histogram(data[column], title=title)\n",
        "end\n",
        "\n",
        "# Function to create a scatter plot\n",
        "function create_scatter(data, x_column, y_column, title)\n",
        "    scatter(data[x_column], data[y_column], title=title)\n",
        "end\n",
        "\n",
        "# Function to create a heatmap\n",
        "function create_heatmap(xlabels, ylabels, matrix, title)\n",
        "    heatmap(xlabels, ylabels, matrix, title=title, show_text=true)\n",
        "end\n",
        "\n",
        "# Function to create a line plot\n",
        "function create_line(data, x_column, y_column, title)\n",
        "    plot(data[x_column], data[y_column], title=title)\n",
        "end\n",
        "\n",
        "\n",
        "end # module Plotting"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E0ojCsPUADaX"
      },
      "source": [
        "# Distributions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x73EWJEDAHOW",
        "outputId": "e4939e86-2752-404e-f9dd-9d0d101ea17c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Main.NormalDistribution"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "module NormalDistribution\n",
        "\n",
        "using Distributions: logpdf, Normal, ContinuousUnivariateDistribution\n",
        "import Distributions: logpdf, Normal, pdf, quantile\n",
        "using Main.MathOps: softplus, inverse_softplus, logsumexp\n",
        "using Random\n",
        "export NormalDist\n",
        "import SpecialFunctions: erfinv\n",
        "const scaling_function = \"softplus\"\n",
        "positive_function, positive_inverse = if scaling_function == \"softplus\"\n",
        "    (softplus, inverse_softplus)\n",
        "elseif scaling_function == \"exponential\"\n",
        "    (exp, log)\n",
        "else\n",
        "    error(\"Scaling function not recognized\")\n",
        "end\n",
        "\n",
        "struct NormalDist{T<:Real} <: ContinuousUnivariateDistribution\n",
        "    loc::T\n",
        "    optim_scale::T\n",
        "    function NormalDist(loc::T, scale::T; validate_args::Bool=false) where {T<:Real}\n",
        "        new{T}(loc, positive_inverse(scale))\n",
        "    end\n",
        "end\n",
        "\n",
        "function logpdf(d::NormalDist, x)\n",
        "    σ = positive_function(d.optim_scale)\n",
        "    return logpdf(Normal(d.loc, σ), x)\n",
        "end\n",
        "\n",
        "function params(d::NormalDist)\n",
        "    return [d.loc, positive_function(d.optim_scale)]\n",
        "end\n",
        "\n",
        "function optim_params(d::NormalDist)\n",
        "    return [d.loc, d.optim_scale]\n",
        "end\n",
        "\n",
        "# keeping a reference to the “inner” constructor\n",
        "const _origNormalDist = NormalDist\n",
        "# outer constructor: accept any Real pair, ignore kwargs, promote to Float64\n",
        "function NormalDist(loc::Real, scale::Real; kwargs...)\n",
        "    return _origNormalDist(float(loc), float(scale))\n",
        "end\n",
        "\n",
        "\n",
        "σ(d) = positive_function(d.optim_scale)\n",
        "\n",
        "rand(rng::Random.AbstractRNG, d::NormalDist) =\n",
        "    d.loc + σ(d) * randn(rng)\n",
        "\n",
        "pdf(d::NormalDist, x) =  exp(logpdf(d, x))\n",
        "\n",
        "quantile(d::NormalDist, p::Real) =\n",
        "    d.loc + σ(d) * √2 * erfinv(2p - 1)\n",
        "\n",
        "end # module NormalDistribution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IqqhGVKPAHKN",
        "outputId": "14fc5dc8-aa04-4e51-be4a-3b52d4000e95"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Main.CustomGamma"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "module CustomGamma\n",
        "\n",
        "using Distributions: logpdf,\n",
        "    Normal, Beta, Exponential, Uniform, Gamma, Dirichlet, Categorical, Bernoulli,\n",
        "    ContinuousUnivariateDistribution, DiscreteUnivariateDistribution,\n",
        "    ContinuousMultivariateDistribution, DiscreteMultivariateDistribution\n",
        "import Distributions: logpdf, Normal\n",
        "using Main.MathOps: softplus, inverse_softplus, logsumexp\n",
        "using Random\n",
        "\n",
        "const scaling_function = \"softplus\"\n",
        "positive_function, positive_inverse = if scaling_function == \"softplus\"\n",
        "    (softplus, inverse_softplus)\n",
        "elseif scaling_function == \"exponential\"\n",
        "    (exp, log)\n",
        "else\n",
        "    error(\"Scaling function not recognized\")\n",
        "end\n",
        "\n",
        "struct GammaDist{T<:Real} <: ContinuousUnivariateDistribution\n",
        "    concentration::T\n",
        "    optim_rate::T\n",
        "    function GammaDist(concentration::T, rate::T) where {T<:Real}\n",
        "        new{T}(concentration, positive_inverse(rate))\n",
        "    end\n",
        "end\n",
        "\n",
        "function logpdf(d::GammaDist, x)\n",
        "    α = positive_function(d.concentration)\n",
        "    β = positive_function(d.optim_rate)\n",
        "    return logpdf(Gamma(α, β), x)\n",
        "end\n",
        "\n",
        "function params(d::GammaDist)\n",
        "    return [positive_function(d.concentration), positive_function(d.optim_rate)]\n",
        "end\n",
        "\n",
        "function optim_params(d::GammaDist)\n",
        "    return [d.concentration, d.optim_rate]\n",
        "end\n",
        "\n",
        "end # module CustomGamma\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DcjtE--jAHF9",
        "outputId": "421a199b-ca64-47c7-ace4-8b40562443e8"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Main.CustomExponential"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "module CustomExponential\n",
        "\n",
        "using Distributions: logpdf,\n",
        "    Normal, Beta, Exponential, Uniform, Gamma, Dirichlet, Categorical, Bernoulli,\n",
        "    ContinuousUnivariateDistribution, DiscreteUnivariateDistribution,\n",
        "    ContinuousMultivariateDistribution, DiscreteMultivariateDistribution\n",
        "import Distributions: logpdf, Normal\n",
        "using Main.MathOps: softplus, inverse_softplus, logsumexp\n",
        "using Random\n",
        "\n",
        "const scaling_function = \"softplus\"\n",
        "positive_function, positive_inverse = if scaling_function == \"softplus\"\n",
        "    (softplus, inverse_softplus)\n",
        "elseif scaling_function == \"exponential\"\n",
        "    (exp, log)\n",
        "else\n",
        "    error(\"Scaling function not recognized\")\n",
        "end\n",
        "\n",
        "struct ExponentialDist{T<:Real} <: ContinuousUnivariateDistribution\n",
        "    optim_rate::T\n",
        "    function ExponentialDist(rate::T) where {T<:Real}\n",
        "        new{T}(positive_inverse(rate))\n",
        "    end\n",
        "end\n",
        "\n",
        "function logpdf(d::ExponentialDist, x)\n",
        "    λ = positive_function(d.optim_rate)\n",
        "    return logpdf(Exponential(λ), x)\n",
        "end\n",
        "\n",
        "function params(d::ExponentialDist)\n",
        "    return [positive_function(d.optim_rate)]\n",
        "end\n",
        "\n",
        "function optim_params(d::ExponentialDist)\n",
        "    return [d.optim_rate]\n",
        "end\n",
        "\n",
        "end # module CustomExponential\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C-PNesm4AS2d",
        "outputId": "afff461e-8196-45f1-e51b-ea39a622bd1b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Main.CustomDirichlet"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "module CustomDirichlet\n",
        "\n",
        "using Distributions: logpdf,\n",
        "    Normal, Beta, Exponential, Uniform, Gamma, Dirichlet, Categorical, Bernoulli,\n",
        "    ContinuousUnivariateDistribution, DiscreteUnivariateDistribution,\n",
        "    ContinuousMultivariateDistribution, DiscreteMultivariateDistribution\n",
        "import Distributions: logpdf, Normal\n",
        "using Main.MathOps: softplus, inverse_softplus, logsumexp\n",
        "using Random\n",
        "\n",
        "const scaling_function = \"softplus\"\n",
        "positive_function, positive_inverse = if scaling_function == \"softplus\"\n",
        "    (softplus, inverse_softplus)\n",
        "elseif scaling_function == \"exponential\"\n",
        "    (exp, log)\n",
        "else\n",
        "    error(\"Scaling function not recognized\")\n",
        "end\n",
        "\n",
        "struct DirichletDist{T<:AbstractVector} <: ContinuousMultivariateDistribution\n",
        "    optim_concentration::T\n",
        "    function DirichletDist(concentration::T) where {T<:AbstractVector}\n",
        "        new{T}(positive_inverse.(concentration))\n",
        "    end\n",
        "end\n",
        "\n",
        "function logpdf(d::DirichletDist, x)\n",
        "    α = positive_function.(d.optim_concentration)\n",
        "    return Distributions.logpdf(Dirichlet(α), x)\n",
        "end\n",
        "\n",
        "function params(d::DirichletDist)\n",
        "    return [positive_function.(d.optim_concentration)]\n",
        "end\n",
        "\n",
        "function optim_params(d::DirichletDist)\n",
        "    return d.optim_concentration\n",
        "end\n",
        "\n",
        "end # module CustomDirichlet\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6aGuRo4rASz8",
        "outputId": "19bbc746-cf5e-43b6-ffff-10a6c540f0e8"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Main.CustomBeta"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "module CustomBeta\n",
        "\n",
        "using Distributions: logpdf,\n",
        "    Normal, Beta, Exponential, Uniform, Gamma, Dirichlet, Categorical, Bernoulli,\n",
        "    ContinuousUnivariateDistribution, DiscreteUnivariateDistribution,\n",
        "    ContinuousMultivariateDistribution, DiscreteMultivariateDistribution\n",
        "import Distributions: logpdf, Normal\n",
        "using Main.MathOps: softplus, inverse_softplus, logsumexp\n",
        "using Random\n",
        "\n",
        "const scaling_function = \"softplus\"\n",
        "positive_function, positive_inverse = if scaling_function == \"softplus\"\n",
        "    (softplus, inverse_softplus)\n",
        "elseif scaling_function == \"exponential\"\n",
        "    (exp, log)\n",
        "else\n",
        "    error(\"Scaling function not recognized\")\n",
        "end\n",
        "\n",
        "struct BetaDist{T<:Real} <: ContinuousUnivariateDistribution\n",
        "    optim_concentration1::T\n",
        "    optim_concentration0::T\n",
        "    function BetaDist(concentration1::T, concentration0::T) where {T<:Real}\n",
        "        new{T}(positive_inverse(concentration1), positive_inverse(concentration0))\n",
        "    end\n",
        "end\n",
        "\n",
        "function logpdf(d::BetaDist, x)\n",
        "    α = positive_function(d.optim_concentration1)\n",
        "    β = positive_function(d.optim_concentration0)\n",
        "    return logpdf(Beta(α, β), x)\n",
        "end\n",
        "\n",
        "function params(d::BetaDist)\n",
        "    return [positive_function(d.optim_concentration1), positive_function(d.optim_concentration0)]\n",
        "end\n",
        "\n",
        "function optim_params(d::BetaDist)\n",
        "    return [d.optim_concentration1, d.optim_concentration0]\n",
        "end\n",
        "\n",
        "end # module CustomBeta\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5AjdqFKRASx_",
        "outputId": "1c46a256-aa9a-483f-9375-973c03daf0d7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Main.CustomCategorical"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "module CustomCategorical\n",
        "\n",
        "using Distributions: logpdf,\n",
        "    Normal, Beta, Exponential, Uniform, Gamma, Dirichlet, Categorical, Bernoulli,\n",
        "    ContinuousUnivariateDistribution, DiscreteUnivariateDistribution,\n",
        "    ContinuousMultivariateDistribution, DiscreteMultivariateDistribution\n",
        "import Distributions: logpdf, Normal\n",
        "using Main.MathOps: softplus, inverse_softplus, logsumexp\n",
        "using Random\n",
        "\n",
        "struct CategoricalDist{T<:AbstractVector} <: DiscreteUnivariateDistribution\n",
        "    logits::T\n",
        "    function CategoricalDist(probs::T) where {T<:AbstractVector}\n",
        "        logits = log.(probs) .- logsumexp(log.(probs))\n",
        "        new{T}(logits .- logsumexp(logits))\n",
        "    end\n",
        "end\n",
        "\n",
        "function logpdf(d::CategoricalDist, x)\n",
        "    return logpdf(Categorical(logits=d.logits), x)\n",
        "end\n",
        "\n",
        "function params(d::CategoricalDist)\n",
        "    return [d.logits]\n",
        "end\n",
        "\n",
        "function optim_params(d::CategoricalDist)\n",
        "    return [d.logits]\n",
        "end\n",
        "\n",
        "end # module CustomCategorical\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WskG5VmfASsW",
        "outputId": "46615b2a-3616-4799-f71b-bd93fcd77ecf"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Main.DiracDeltaDistribution"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "module DiracDeltaDistribution\n",
        "\n",
        "import Random: rand\n",
        "using Distributions: logpdf,\n",
        "    Normal, Beta, Exponential, Uniform, Gamma, Dirichlet, Categorical, Bernoulli,\n",
        "    ContinuousUnivariateDistribution, DiscreteUnivariateDistribution,\n",
        "    ContinuousMultivariateDistribution, DiscreteMultivariateDistribution\n",
        "import Distributions: logpdf, Normal\n",
        "using Main.MathOps: softplus, inverse_softplus, logsumexp\n",
        "using Random\n",
        "\n",
        "# Type representing a delta distribution\n",
        "struct DeltaDistribution\n",
        "    x::Any\n",
        "end\n",
        "\n",
        "rand(::Random.AbstractRNG, dist::DeltaDistribution) = dist.x\n",
        "rand(dist::DeltaDistribution)                      = dist.x\n",
        "\n",
        "# Log probability for the delta distribution\n",
        "function logpdf(d::DeltaDistribution, x)\n",
        "    return d.x == x ? Inf : -Inf\n",
        "end\n",
        "\n",
        "# Function to return different types of distributions based on the scheme\n",
        "function dirac_delta_distribution(x...; scheme=\"normal\")\n",
        "    if scheme == \"normal\"\n",
        "        return Normal(x[1], 0.1)\n",
        "    elseif scheme == \"uniform\"\n",
        "        return Uniform(x[1] - 0.05, x[1] + 0.05)\n",
        "    elseif scheme == \"delta\"\n",
        "        return DeltaDistribution(x[1])\n",
        "    else\n",
        "        error(\"Dirac delta scheme not recognized\")\n",
        "    end\n",
        "end\n",
        "\n",
        "end # module DiracDeltaDistribution\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1SZf_fvHASlW",
        "outputId": "be670faf-f32d-4709-acf0-30632cda648c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Main.CustomBernoulli"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "module CustomBernoulli\n",
        "\n",
        "using Distributions: logpdf,\n",
        "    Normal, Beta, Exponential, Uniform, Gamma, Dirichlet, Categorical, Bernoulli,\n",
        "    ContinuousUnivariateDistribution, DiscreteUnivariateDistribution,\n",
        "    ContinuousMultivariateDistribution, DiscreteMultivariateDistribution\n",
        "import Distributions: logpdf, Normal\n",
        "using Main.MathOps: softplus, inverse_softplus, logsumexp\n",
        "using Random\n",
        "\n",
        "struct BernoulliDist{T<:Real} <: DiscreteUnivariateDistribution\n",
        "    logits::T\n",
        "    function BernoulliDist(probs::T) where {T<:Real}\n",
        "        new{T}(log(probs / (1 - probs)))\n",
        "    end\n",
        "end\n",
        "\n",
        "function logpdf(d::BernoulliDist, x)\n",
        "    return logpdf(Bernoulli(logits=d.logits), x)\n",
        "end\n",
        "\n",
        "function params(d::BernoulliDist)\n",
        "    return [d.logits]\n",
        "end\n",
        "\n",
        "function optim_params(d::BernoulliDist)\n",
        "    return [d.logits]\n",
        "end\n",
        "\n",
        "end # module CustomBernoulli\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Qh0TnQyAunu"
      },
      "source": [
        "# Main"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Dt5EslIA0HG",
        "outputId": "30312e44-8dcb-4e17-943d-63ca6b636af3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Main.DistributionsPrep"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "module DistributionsPrep\n",
        "\n",
        "using Distributions: Normal, Beta, Exponential, Uniform, Categorical, Bernoulli, Gamma, Dirichlet\n",
        "using StatsFuns: logsumexp\n",
        "\n",
        "using Random\n",
        "\n",
        "# Import custom distributions\n",
        "using Main.NormalDistribution: NormalDist\n",
        "using Main.CustomGamma: GammaDist\n",
        "using Main.CustomBeta: BetaDist\n",
        "using Main.CustomExponential: ExponentialDist\n",
        "using Main.CustomDirichlet: DirichletDist\n",
        "using Main.CustomCategorical: CategoricalDist\n",
        "using Main.CustomBernoulli: BernoulliDist\n",
        "using Main.DiracDeltaDistribution: dirac_delta_distribution\n",
        "\n",
        "# List of all supported distributions\n",
        "distributions = [\n",
        "    \"normal\",\n",
        "    \"beta\",\n",
        "    \"exponential\",\n",
        "    \"uniform-continuous\",\n",
        "    \"discrete\",\n",
        "    \"bernoulli\",\n",
        "    \"gamma\",\n",
        "    \"dirichlet\",\n",
        "    \"flip\",\n",
        "    \"dirac\"\n",
        "]\n",
        "\n",
        "# Dictionary mapping distribution names to Julia distribution constructors\n",
        "distribution_constructors = Dict(\n",
        "    \"normal\" => NormalDist,\n",
        "    \"beta\" => BetaDist,\n",
        "    \"exponential\" => ExponentialDist,\n",
        "    \"uniform-continuous\" => Uniform,\n",
        "    \"discrete\" => CategoricalDist,\n",
        "    \"bernoulli\" => BernoulliDist,\n",
        "    \"gamma\" => GammaDist,\n",
        "    \"dirichlet\" => DirichletDist,\n",
        "    \"flip\" => BernoulliDist,\n",
        "    \"dirac\" => dirac_delta_distribution\n",
        ")\n",
        "\n",
        "# Starting parameters for distributions for necessary cases\n",
        "distribution_params = Dict(\n",
        "    \"normal-params\" => (0.0, 1.0),\n",
        "    \"beta-params\" => (1.0, 1.0),\n",
        "    \"exponential-params\" => (1.0,),\n",
        "    \"uniform-continuous-params\" => (0.0, 1.0),\n",
        "    \"discrete-params\" => ([1/3, 1/3, 1/3],),\n",
        "    \"bernoulli-params\" => (0.5,),\n",
        "    \"gamma-params\" => (1.0, 1.0),\n",
        "    \"dirichlet-params\" => ([1.0, 1.0, 1.0],),\n",
        "    \"flip-params\" => (0.5,)\n",
        ")\n",
        "\n",
        "end # module DistributionsPrep\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nF7NgpVlAv1f",
        "outputId": "14490222-565d-40ea-81e2-644d38fbcec1"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Main.BasicPrimitives"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "module BasicPrimitives\n",
        "\n",
        "using Random\n",
        "using LinearAlgebra\n",
        "using Distributions\n",
        "import Main.DistributionsPrep: distribution_constructors\n",
        "\n",
        "# helper functions\n",
        "\n",
        "vector(xs...) = collect(xs)\n",
        "\n",
        "function _get(container, idx)\n",
        "    if container isa AbstractArray\n",
        "        return container[Int(idx)+1]\n",
        "    elseif container isa Dict\n",
        "        return container[idx]\n",
        "    else\n",
        "        error(\"`get` not defined for $(typeof(container))\")\n",
        "    end\n",
        "end\n",
        "\n",
        "function _put(container, idx, val)\n",
        "    if container isa AbstractArray\n",
        "        container[Int(idx)+1] = val\n",
        "        return container\n",
        "    elseif container isa Dict\n",
        "        container[idx] = val\n",
        "        return container\n",
        "    else\n",
        "        error(\"`put` not defined for $(typeof(container))\")\n",
        "    end\n",
        "end\n",
        "\n",
        "_append(a::AbstractVector, v) = (push!(a, v); a)\n",
        "hashmap(kv...) = Dict(kv...)\n",
        "_and(a,b) = a && b\n",
        "_or(a,b)  = a || b\n",
        "_mat_tanh(M) = tanh.(M)\n",
        "\n",
        "# Convert a nested vector to a proper Matrix{Float64}\n",
        "function to_matrix(V)\n",
        "    V isa AbstractMatrix && return V\n",
        "    rows = length(V)\n",
        "    cols = length(V[1])\n",
        "    M = Array{Float64}(undef, rows, cols)\n",
        "    @inbounds for i in 1:rows, j in 1:cols\n",
        "        M[i,j] = Float64(V[i][j])\n",
        "    end\n",
        "    return M\n",
        "end\n",
        "\n",
        "# Create a zero-based categorical distribution\n",
        "function zero_based_categorical(p::AbstractVector{<:Real})\n",
        "    cats = collect(0:length(p)-1)    # categories 0,1,2,...\n",
        "    return DiscreteNonParametric(cats, p)\n",
        "end\n",
        "\n",
        "\n",
        "const primitives = Dict{String,Any}(\n",
        "\n",
        "    # logic & comparison\n",
        "    \"<\"   => (<),\n",
        "    \"<=\"  => (<=),\n",
        "    \">\"   => (>),\n",
        "    \">=\"  => (>=),\n",
        "    \"=\"   => (==),\n",
        "    \"and\" => _and,\n",
        "    \"or\"  => _or,\n",
        "\n",
        "    # arithmetic\n",
        "    \"+\"   => (+),\n",
        "    \"-\"   => (-),\n",
        "    \"*\"   => (*),\n",
        "    \"/\"   => (/),\n",
        "    \"exp\" => exp,\n",
        "    \"sqrt\"=> sqrt,\n",
        "    \"abs\" => abs,\n",
        "\n",
        "    # container ops\n",
        "    \"vector\" => vector,\n",
        "    \"get\"    => _get,\n",
        "    \"put\"    => _put,\n",
        "    \"append\" => _append,\n",
        "    \"first\"  => xs -> xs[1],\n",
        "    \"second\" => xs -> xs[2],\n",
        "    \"last\"   => xs -> xs[end],\n",
        "    \"rest\"   => xs -> xs[2:end],\n",
        "    \"hash-map\" => hashmap,\n",
        "\n",
        "    # matrix ops\n",
        "    \"mat-mul\"       => (A,B)   -> to_matrix(A) * to_matrix(B),\n",
        "    \"mat-add\"       => (A,B)   -> to_matrix(A) .+ to_matrix(B),\n",
        "    \"mat-transpose\" => M       -> transpose(to_matrix(M)),\n",
        "    \"mat-repmat\"    => (M,r,c) -> repeat(to_matrix(M), (Int(r),Int(c))),\n",
        "    \"mat-tanh\"      => _mat_tanh,\n",
        "\n",
        "    # “normal” primitive\n",
        "    \"normal\"        => (μ, σ; kwargs...) -> Normal(float(μ), float(σ)),\n",
        "\n",
        "    # discrete / categorical\n",
        "    \"discrete\"       => (p; kwargs...) -> zero_based_categorical(p),\n",
        "    \"discrete-guide\" => (p; kwargs...) -> zero_based_categorical(p),\n",
        "\n",
        "    # all distribution constructors and their guides\n",
        "    distribution_constructors...,\n",
        "    [k * \"-guide\" => v for (k,v) in distribution_constructors]...,\n",
        "\n",
        "    # domain-specific stub\n",
        "    \"oneplanet\" => (args...)->error(\"`oneplanet` not implemented in Julia backend.\")\n",
        ")\n",
        "\n",
        "export primitives\n",
        "\n",
        "end # module\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U5mM2wJEAE-c"
      },
      "source": [
        "# Inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w3HJHTDyADDF",
        "outputId": "38752328-2b6d-425d-c710-6a686dfb3c2e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Main.EvaluationBasedSampling"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "module EvaluationBasedSampling\n",
        "\n",
        "using Random\n",
        "using Statistics\n",
        "using LinearAlgebra\n",
        "using Main.BasicPrimitives: primitives\n",
        "using Main.DistributionsPrep: distributions\n",
        "using Distributions\n",
        "\n",
        "export AbstractSyntaxTree, eval, bind_functions, evaluate_program\n",
        "\n",
        "# Define the AbstractSyntaxTree type\n",
        "struct AbstractSyntaxTree\n",
        "    functions::Vector{Any}\n",
        "    program::Any\n",
        "\n",
        "    function AbstractSyntaxTree(ast_json::Vector{Any})\n",
        "        new(ast_json[1:end-1], ast_json[end])\n",
        "    end\n",
        "end\n",
        "\n",
        "# Evaluate function\n",
        "function eval(e, sig, l, rho=Dict{String,Any}(); verbose=false)\n",
        "    if verbose println(\"Expression (before): \", e) end\n",
        "\n",
        "    if e isa Number || e isa Bool\n",
        "        result = e\n",
        "\n",
        "    elseif e isa String\n",
        "        result = l[e]\n",
        "\n",
        "    elseif e isa Array\n",
        "        if e[1] == \"defn\"\n",
        "            error(\"This defn case should never happen!\")\n",
        "\n",
        "        elseif e[1] == \"let\"\n",
        "            expression, name = e[2][2], e[2][1]\n",
        "            c1 = eval(expression, sig, l, rho)\n",
        "            l[name] = c1\n",
        "            result = eval(e[3], sig, l, rho)\n",
        "\n",
        "        elseif e[1] == \"if\"\n",
        "            e1 = eval(e[2], sig, l, rho)\n",
        "            result = e1 ? eval(e[3], sig, l, rho) : eval(e[4], sig, l, rho)\n",
        "\n",
        "        elseif e[1] in [\"sample\", \"sample*\"]\n",
        "            d = eval(e[2], sig, l, rho)\n",
        "            s = rand(d)\n",
        "            log_prob = logpdf(d, s)\n",
        "            sig[\"logP\"] += log_prob\n",
        "            result = s\n",
        "\n",
        "        elseif e[1] in [\"observe\", \"observe*\"]\n",
        "            d = eval(e[2], sig, l, rho)\n",
        "            y = eval(e[3], sig, l, rho)\n",
        "            log_prob = logpdf(d, y)\n",
        "            sig[\"logP\"] += log_prob\n",
        "            sig[\"logW\"] += log_prob\n",
        "            result = y\n",
        "\n",
        "        else\n",
        "            cs = [eval(element, sig, l, rho) for element in e[2:end]]\n",
        "\n",
        "            if e[1] isa Array\n",
        "                println(\"List: \", e[1])\n",
        "                error(\"This list case should never happen!\")\n",
        "\n",
        "            elseif (e[1] isa String) && (haskey(rho, e[1]))\n",
        "                variables, function_body = rho[e[1]]\n",
        "                func_env = deepcopy(l)\n",
        "                for (variable, exp) in zip(variables, cs)\n",
        "                    func_env[variable] = exp\n",
        "                end\n",
        "                func_env[e[1]] = function_body\n",
        "                result = eval(function_body, sig, func_env, rho)\n",
        "\n",
        "            elseif (e[1] isa String) && (e[1] in distributions) && (haskey(primitives, e[1]))\n",
        "                result = primitives[e[1]](cs...; validate_args=false)\n",
        "\n",
        "            elseif (e[1] isa String) && (haskey(primitives, e[1]))\n",
        "                result = primitives[e[1]](cs...)\n",
        "\n",
        "            else\n",
        "                println(\"List expression not recognised: \", e)\n",
        "                error(\"List expression not recognised\")\n",
        "            end\n",
        "        end\n",
        "\n",
        "    else\n",
        "        println(\"Expression not recognised: \", e)\n",
        "        error(\"Expression not recognised\")\n",
        "    end\n",
        "\n",
        "    if verbose\n",
        "        println(\"Expression (after): \", e)\n",
        "        println(\"Result: \", result, typeof(result))\n",
        "    end\n",
        "\n",
        "    return result\n",
        "end\n",
        "\n",
        "# Bind functions\n",
        "function bind_functions(ast::AbstractSyntaxTree)\n",
        "    rho = Dict{String,Any}()\n",
        "    for e in ast.functions\n",
        "        if e[1] == \"defn\"\n",
        "            rho[e[2]] = (e[3], e[4])\n",
        "        end\n",
        "    end\n",
        "    return rho\n",
        "end\n",
        "\n",
        "# Evaluate program\n",
        "function evaluate_program(ast::AbstractSyntaxTree; verbose=false)\n",
        "    sig = Dict(\"logW\" => 0.0, \"logP\" => 0.0)\n",
        "    l = Dict{String,Any}()\n",
        "    rho = bind_functions(ast)\n",
        "    e = eval(ast.program, sig, l, rho; verbose=verbose)\n",
        "    return e, sig, l\n",
        "end\n",
        "\n",
        "end # module EvaluationBasedSampling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AWLNcPSYIizY",
        "outputId": "f36a1726-fd70-4d89-c45f-0f0129a9a916"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Main.MiniTopologicalSorter"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "module MiniTopologicalSorter\n",
        "\n",
        "export TopologicalSorter, static_order\n",
        "\n",
        "function _kahn_order(arrows::Dict)\n",
        "    # make sure every node appears as a key\n",
        "    for v in values(arrows), child in v\n",
        "        arrows[child] = get(arrows, child, [])\n",
        "    end\n",
        "\n",
        "    # compute in-degrees\n",
        "    indeg = Dict(n => 0 for n in keys(arrows))\n",
        "    for v in values(arrows), child in v\n",
        "        indeg[child] += 1\n",
        "    end\n",
        "\n",
        "    # queue all zero-in-degree nodes\n",
        "    ready  = [n for n in keys(indeg) if indeg[n] == 0]\n",
        "    sorted = Any[]\n",
        "\n",
        "    # Kahn’s loop\n",
        "    while !isempty(ready)\n",
        "        n = popfirst!(ready)\n",
        "        push!(sorted, n)\n",
        "\n",
        "        for child in arrows[n]\n",
        "            indeg[child] -= 1\n",
        "            indeg[child] == 0 && push!(ready, child)\n",
        "        end\n",
        "    end\n",
        "\n",
        "    length(sorted) == length(indeg) ||\n",
        "        throw(ErrorException(\"cycle detected – graph is not a DAG\"))\n",
        "\n",
        "    return sorted        # parents → children\n",
        "end\n",
        "\n",
        "struct TopologicalSorter\n",
        "    arrows::Dict{Any,Vector{Any}}\n",
        "    order::Vector{Any}         # cached parent→child order\n",
        "    function TopologicalSorter(arrows::Dict)\n",
        "        new(arrows, _kahn_order(deepcopy(arrows)))\n",
        "    end\n",
        "end\n",
        "\n",
        "\"\"\"\n",
        "Return an iterator that yields the nodes in the same order as\n",
        "`ts.order`.\n",
        "\"\"\"\n",
        "static_order(ts::TopologicalSorter) =\n",
        "    (n for n in ts.order)\n",
        "\n",
        "end # module MiniTopologicalSorter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gg_smUZ0ADEu",
        "outputId": "5380253d-b1c3-4927-d88d-06c5de2349ba"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING: import of EvaluationBasedSampling.eval into GraphBasedSamplingUtils conflicts with an existing identifier; ignored.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "Main.GraphBasedSamplingUtils"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "module GraphBasedSamplingUtils\n",
        "\n",
        "using Random\n",
        "using Statistics\n",
        "using LinearAlgebra\n",
        "using Printf\n",
        "using Main.MiniTopologicalSorter: TopologicalSorter\n",
        "using Main.BasicPrimitives: primitives\n",
        "using Main.EvaluationBasedSampling: eval\n",
        "using Zygote: gradient\n",
        "using Optimisers: Adam\n",
        "using Optimisers\n",
        "\n",
        "export Graph, evaluate_node, evaluate_graph, generate_IC, log_joint\n",
        "\n",
        "# Define the Graph type\n",
        "mutable struct Graph\n",
        "    functions::Vector{Any}\n",
        "    nodes::Vector{Any}\n",
        "    arrows::Dict{Any, Any}\n",
        "    expressions::Dict{Any, Any}\n",
        "    observe::Any\n",
        "    program::Any\n",
        "\n",
        "    function Graph(graph_json::Vector{Any})\n",
        "        g = new(\n",
        "            graph_json[1],\n",
        "            graph_json[2][\"V\"],\n",
        "            graph_json[2][\"A\"],\n",
        "            graph_json[2][\"P\"],\n",
        "            graph_json[2][\"Y\"],\n",
        "            graph_json[3]\n",
        "        )\n",
        "        g.nodes = topological_sort(g)\n",
        "        return g\n",
        "    end\n",
        "end\n",
        "\n",
        "function topological_sort(g::Graph; verbose=false)\n",
        "    for node in g.nodes\n",
        "        if !haskey(g.arrows, node)\n",
        "            g.arrows[node] = []\n",
        "        end\n",
        "    end\n",
        "    if verbose println(\"arrows: \", g.arrows) end\n",
        "\n",
        "    sorter = TopologicalSorter(g.arrows)\n",
        "    sorted_list = collect(sorter)\n",
        "    return reverse(sorted_list)\n",
        "end\n",
        "\n",
        "function split_nodes_into_sample_observe(g::Graph)\n",
        "    sample_nodes, observe_nodes = [], []\n",
        "    for node in g.nodes\n",
        "        if occursin(\"sample\", node)\n",
        "            push!(sample_nodes, node)\n",
        "        elseif occursin(\"observe\", node)\n",
        "            push!(observe_nodes, node)\n",
        "        else\n",
        "            error(\"Node present that is neither sample nor observe\")\n",
        "        end\n",
        "    end\n",
        "    return sample_nodes, observe_nodes\n",
        "end\n",
        "\n",
        "### Evaluation ###\n",
        "\n",
        "function evaluate_node(node, exp, sig, l; fixed_dists=Dict(), fixed_nodes=Dict(), fixed_probs=Dict(), verbose=false)\n",
        "    if verbose println(\"Node: \", node) end\n",
        "    if haskey(fixed_dists, node)\n",
        "        result = rand(fixed_dists[node])\n",
        "        p_log_prob = logpdf(eval(exp[2], sig, l), result)\n",
        "        q_log_prob = logpdf(fixed_dists[node], result)\n",
        "        sig[\"logP\"] += q_log_prob\n",
        "        sig[\"logW\"] += p_log_prob - q_log_prob\n",
        "    elseif haskey(fixed_nodes, node) && haskey(fixed_probs, node)\n",
        "        result = fixed_nodes[node]\n",
        "        log_prob = fixed_probs[node]\n",
        "        sig[\"logP\"] += log_prob\n",
        "        if occursin(\"observe\", node)\n",
        "            sig[\"logW\"] += log_prob\n",
        "        end\n",
        "    elseif haskey(fixed_nodes, node)\n",
        "        result = fixed_nodes[node]\n",
        "        log_prob = logpdf(eval(exp[2], sig, l), result)\n",
        "        sig[\"logP\"] += log_prob\n",
        "        if occursin(\"observe\", node)\n",
        "            sig[\"logW\"] += log_prob\n",
        "        end\n",
        "    else\n",
        "        result = eval(exp, sig, l, verbose=verbose)\n",
        "    end\n",
        "    if verbose println(\"Value: \", result) end\n",
        "    return result\n",
        "end\n",
        "\n",
        "function evaluate_graph(g::Graph; fixed_dists=Dict(), fixed_nodes=Dict(), fixed_probs=Dict(), verbose=false)\n",
        "    if verbose println(g) end\n",
        "\n",
        "    sig = Dict(\"logW\" => 0.0, \"logP\" => 0.0)\n",
        "    l = Dict{Any, Any}()\n",
        "    for node in g.nodes\n",
        "        exp = g.expressions[node]\n",
        "        original_logP = sig[\"logP\"]\n",
        "        result = evaluate_node(node, exp, sig, l, fixed_dists=fixed_dists, fixed_nodes=fixed_nodes, fixed_probs=fixed_probs, verbose=verbose)\n",
        "        l[node] = result\n",
        "        l[node * \"_logP\"] = sig[\"logP\"] - original_logP\n",
        "    end\n",
        "\n",
        "    result = eval(g.program, sig, l, verbose=verbose)\n",
        "    if verbose println(\"Result: \", result) end\n",
        "    return result, sig, l\n",
        "end\n",
        "\n",
        "### Hamiltonian Monte Carlo ###\n",
        "\n",
        "function generate_IC(g::Graph; verbose=false)\n",
        "    _, _, l = evaluate_graph(g, verbose=verbose)\n",
        "    start = [l[node] for node in g.nodes if occursin(\"sample\", node)]\n",
        "    if verbose println(\"Initial conditions: \", start) end\n",
        "    return start\n",
        "end\n",
        "\n",
        "function log_joint(g::Graph, x; verbose=false)\n",
        "    fixed_nodes = Dict()\n",
        "    i = 1\n",
        "    for node in g.nodes\n",
        "        if occursin(\"sample\", node)\n",
        "            fixed_nodes[node] = x[i]\n",
        "            i += 1\n",
        "        end\n",
        "    end\n",
        "    _, sig, _ = evaluate_graph(g, fixed_nodes=fixed_nodes, verbose=verbose)\n",
        "    log_joint = sig[\"logP\"]\n",
        "    return log_joint\n",
        "end\n",
        "\n",
        "### Variational Inference Utilities ###\n",
        "\n",
        "function save_parameters(parameters::Vector, variationals::Dict)\n",
        "    params_here = []\n",
        "    for dist in values(variationals)\n",
        "        params = [deepcopy(p) for p in dist.params()]\n",
        "        append!(params_here, params)\n",
        "    end\n",
        "    push!(parameters, params_here)\n",
        "    return parameters\n",
        "end\n",
        "\n",
        "function calculate_b(node::String, variational, logQs::Vector, logWs::Vector; zero=false)\n",
        "    if zero\n",
        "        b = 0.0\n",
        "    else\n",
        "        Fs, Gs = [], []\n",
        "        for (logQ, logW) in zip(logQs, logWs)\n",
        "            Q = logQ[node]\n",
        "            grads = gradient(() -> Q, variational.optim_params())[1]\n",
        "            G = length(grads) == 1 ? grads[1] : collect(grads)\n",
        "            for param in variational.optim_params()\n",
        "                zero!(param.grad)\n",
        "            end\n",
        "            F = G * logW\n",
        "            push!(Fs, F)\n",
        "            push!(Gs, G)\n",
        "        end\n",
        "        Fs = vcat(Fs...)\n",
        "        Gs = vcat(Gs...)\n",
        "        cov_FG = sum(covariance(Fs, Gs))\n",
        "        var_GG = sum(covariance(Gs, Gs))\n",
        "        b = var_GG == 0 ? 0.0 : cov_FG / var_GG\n",
        "    end\n",
        "    return b\n",
        "end\n",
        "\n",
        "function update_parameters(nodes::Vector{String}, variationals::Dict, logQs::Vector, logWs::Vector, optimizer; zero_b=false)\n",
        "    total_ELBO, total_loss = 0.0, 0.0\n",
        "    batch_size = length(logQs)\n",
        "    for node in nodes\n",
        "        b = calculate_b(node, variationals[node], logQs, logWs; zero=zero_b)\n",
        "        ELBO, loss = 0.0, 0.0\n",
        "        for (logQ, logW) in zip(logQs, logWs)\n",
        "            ELBO -= logQ[node] * logW\n",
        "            loss -= logQ[node] * (logW - b)\n",
        "        end\n",
        "        ELBO /= batch_size\n",
        "        loss /= batch_size\n",
        "        total_ELBO += ELBO\n",
        "        total_loss += loss\n",
        "    end\n",
        "    gs = gradient(() -> total_loss, optim.params)[1]\n",
        "    optim.state = Optimisers.update!(optim.opt, optim.state, optim.params, gs)\n",
        "    return deepcopy(total_ELBO)\n",
        "end\n",
        "\n",
        "function initialize_optimizer(variationals::Dict, learning_rate::Float64)\n",
        "    all_params = Float32[]\n",
        "    for dist in values(variationals)\n",
        "        append!(all_params, dist.optim_params())\n",
        "    end\n",
        "    opt = Adam(learning_rate)\n",
        "    state = Optimisers.setup(opt, all_params)\n",
        "    return (opt=opt, state=state, params=all_params)\n",
        "end\n",
        "\n",
        "end # module GraphBasedSamplingUtils\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h9iBu96fADBV",
        "outputId": "ec8b23ac-5e0c-4047-8154-40870a06ebc4"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Main.GibbsSampling"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "module GibbsSampling\n",
        "\n",
        "using Random\n",
        "using Statistics\n",
        "using LinearAlgebra\n",
        "using Printf\n",
        "using Main.GraphBasedSamplingUtils: Graph, evaluate_graph, split_nodes_into_sample_observe, evaluate_node\n",
        "using Main.SampleHandling: burn_chain, flatten_sample\n",
        "#using Main.Plotting: log_sample\n",
        "\n",
        "export Gibbs_samples\n",
        "\n",
        "### MH within Gibbs ###\n",
        "\n",
        "function Gibbs_samples(g::Graph, num_samples; tmax=nothing, burn_frac=nothing, wandb_name=nothing, debug=false, verbose=false)\n",
        "    sample_nodes, _ = split_nodes_into_sample_observe(g)\n",
        "\n",
        "    samples, weights = [], []\n",
        "    accepted_small_steps = 0; num_small_steps = 0; num_big_steps = 0\n",
        "    max_time = tmax !== nothing ? time() + tmax : nothing\n",
        "\n",
        "    for i in 1:num_samples\n",
        "        if i == 1\n",
        "            result, sig, l = evaluate_graph(g, verbose=verbose)\n",
        "        else\n",
        "            for resample_node in sample_nodes\n",
        "                resample_logP = l[resample_node * \"_logP\"]\n",
        "                sig_here, l_here = deepcopy(sig), deepcopy(l)\n",
        "                d = eval(g.expressions[resample_node], sig_here, l_here)\n",
        "                resample_logP_new = sig_here[\"logP\"] - sig[\"logP\"]\n",
        "                fixed_nodes, fixed_probs = Dict(resample_node => d), Dict(resample_node => resample_logP_new)\n",
        "                if debug\n",
        "                    println(\"Original node value: \", l[resample_node])\n",
        "                    println(\"Original node logP: \", resample_logP)\n",
        "                    println(\"Resampled node value: \", d)\n",
        "                    println(\"Resampled node logP: \", resample_logP_new)\n",
        "                end\n",
        "\n",
        "                for node in g.nodes\n",
        "                    if node != resample_node\n",
        "                        fixed_nodes[node] = l[node]\n",
        "                        if !(node in g.arrows[resample_node])\n",
        "                            fixed_probs[node] = l[node * \"_logP\"]\n",
        "                        end\n",
        "                    end\n",
        "                end\n",
        "                if debug\n",
        "                    println(\"Fixed nodes: \", fixed_nodes)\n",
        "                    println(\"Fixed probabilities: \", fixed_probs)\n",
        "                end\n",
        "                result_new, sig_new, l_new = evaluate_graph(g, fixed_nodes=fixed_nodes, fixed_probs=fixed_probs, verbose=verbose)\n",
        "                if debug\n",
        "                    println(\"Old sig: \", sig)\n",
        "                    println(\"New sig: \", sig_new)\n",
        "                    println(\"Old environment: \", l)\n",
        "                    println(\"New environment: \", l_new)\n",
        "                end\n",
        "\n",
        "                acceptance = exp(sig_new[\"logP\"] - sig[\"logP\"] - resample_logP_new + resample_logP)\n",
        "                alpha = min(1.0, acceptance)\n",
        "                accept = rand() < alpha\n",
        "                if accept\n",
        "                    result, sig, l = result_new, sig_new, l_new\n",
        "                    accepted_small_steps += 1\n",
        "                end\n",
        "                if wandb_name !== nothing\n",
        "                    log_sample(result, i, wandb_name)\n",
        "                end\n",
        "                num_small_steps += 1\n",
        "                if debug\n",
        "                    break\n",
        "                end\n",
        "            end\n",
        "            if debug\n",
        "                break\n",
        "            end\n",
        "        end\n",
        "\n",
        "        num_big_steps += 1\n",
        "        push!(samples, result)\n",
        "        push!(weights, 1.0)\n",
        "        if tmax !== nothing && time() > max_time\n",
        "            break\n",
        "        end\n",
        "    end\n",
        "\n",
        "    @printf(\"Acceptance fraction: %.3f\\n\", accepted_small_steps / num_small_steps)\n",
        "    println(\"Number of samples: \", num_big_steps)\n",
        "    if burn_frac !== nothing\n",
        "        println(\"Burn fraction: \", burn_frac)\n",
        "        nburn = Int(burn_frac * num_big_steps)\n",
        "        println(\"Burning up to: \", nburn)\n",
        "        samples, weights = burn_chain(samples, weights, burn_frac)\n",
        "    end\n",
        "\n",
        "    return samples, weights\n",
        "end\n",
        "\n",
        "end # module GibbsSampling\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uOIghHRLADJg",
        "outputId": "83f6aacb-d362-4ca1-d38c-d0c7b111150f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Main.GeneralSampling"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "module GeneralSampling\n",
        "\n",
        "using Random\n",
        "using Statistics\n",
        "using LinearAlgebra\n",
        "using Printf\n",
        "using Main.EvaluationBasedSampling: evaluate_program\n",
        "using Main.GraphBasedSamplingUtils: evaluate_graph\n",
        "using Main.SampleHandling: burn_chain, flatten_sample\n",
        "# using Main.Plotting: log_sample\n",
        "\n",
        "export get_sample, prior_samples, calculate_effective_sample_size, resample_using_importance_weights, Metropolis_Hastings_samples\n",
        "\n",
        "function get_sample(ast_or_graph, mode::String; verbose=false)\n",
        "    if mode == \"desugar\"\n",
        "        ret, sig, _ = evaluate_program(ast_or_graph, verbose=verbose)\n",
        "    elseif mode == \"graph\"\n",
        "        ret, sig, _ = evaluate_graph(ast_or_graph, verbose=verbose)\n",
        "    else\n",
        "        error(\"Mode not recognised\")\n",
        "    end\n",
        "    ret = flatten_sample(ret)\n",
        "    return ret, sig\n",
        "end\n",
        "\n",
        "function prior_samples(ast_or_graph, mode::String, num_samples::Int; tmax=nothing, wandb_name=nothing, verbose=false)\n",
        "    samples, weights = [], []\n",
        "    max_time = tmax !== nothing ? time() + tmax : nothing\n",
        "    for i in 1:num_samples\n",
        "        sample, sig = get_sample(ast_or_graph, mode, verbose)\n",
        "        weight = sig[\"logW\"]\n",
        "        if wandb_name !== nothing\n",
        "            log_sample(sample, i, wandb_name=wandb_name)\n",
        "        end\n",
        "        push!(samples, sample)\n",
        "        push!(weights, weight)\n",
        "        if tmax !== nothing && time() > max_time\n",
        "            break\n",
        "        end\n",
        "    end\n",
        "    return samples, weights\n",
        "end\n",
        "\n",
        "function calculate_effective_sample_size(weights::Vector; verbose=false)\n",
        "    norm_weights = weights ./ sum(weights)\n",
        "    ESS = 1.0 / sum(norm_weights .^ 2)\n",
        "    if verbose\n",
        "        println(\"Effective sample size: \", ESS)\n",
        "        println(\"Fractional sample size: \", ESS / length(norm_weights))\n",
        "        println(\"Sum of weights: \", sum(norm_weights))\n",
        "    end\n",
        "    return ESS\n",
        "end\n",
        "\n",
        "function resample_using_importance_weights(samples, log_weights; normalize=true, wandb_name=nothing)\n",
        "    nsamples = size(samples, 1)\n",
        "    if normalize\n",
        "        log_weights = log_weights .- maximum(log_weights)\n",
        "    end\n",
        "    weights = exp.(log_weights)\n",
        "    ESS = calculate_effective_sample_size(weights, verbose=true)\n",
        "    indices = sample(1:nsamples, Weights(weights), nsamples)\n",
        "    new_samples = samples[indices, :]\n",
        "    if wandb_name !== nothing\n",
        "        for (i, sample) in enumerate(new_samples)\n",
        "            log_sample(sample, i, wandb_name, resample=true)\n",
        "        end\n",
        "    end\n",
        "    return new_samples\n",
        "end\n",
        "\n",
        "function Metropolis_Hastings_samples(ast_or_graph, mode::String, num_samples::Int; tmax=nothing, burn_frac=nothing, wandb_name=nothing, verbose=false)\n",
        "    accepted_steps = 0\n",
        "    num_steps = 0\n",
        "    samples, weights = [], []\n",
        "    max_time = tmax !== nothing ? time() + tmax : nothing\n",
        "    old_sample, old_prob = nothing, nothing\n",
        "    for i in 1:num_samples\n",
        "        sample, sig = get_sample(ast_or_graph, mode, verbose)\n",
        "        prob = exp(sig[\"logW\"])\n",
        "        if i != 1\n",
        "            acceptance = min(1.0, prob / old_prob)\n",
        "            accept = rand() < acceptance\n",
        "        else\n",
        "            accept = true\n",
        "        end\n",
        "        if accept\n",
        "            new_sample = sample\n",
        "            new_prob = prob\n",
        "            accepted_steps += 1\n",
        "        else\n",
        "            new_sample = old_sample\n",
        "            new_prob = old_prob\n",
        "        end\n",
        "        num_steps += 1\n",
        "        if wandb_name !== nothing\n",
        "            log_sample(sample, i, wandb_name)\n",
        "        end\n",
        "        push!(samples, new_sample)\n",
        "        push!(weights, 1.0)\n",
        "        old_sample, old_prob = new_sample, new_prob\n",
        "        if tmax !== nothing && time() > max_time\n",
        "            break\n",
        "        end\n",
        "    end\n",
        "    println(\"Acceptance fraction: \", accepted_steps / num_steps)\n",
        "    samples, weights = burn_chain(samples, weights, burn_frac=burn_frac)\n",
        "    return samples, weights\n",
        "end\n",
        "\n",
        "end # module GeneralSampling\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZBP-1eHQK87B"
      },
      "source": [
        "# Tests"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G6YrAwKmAC9-",
        "outputId": "3071fba9-e428-4062-88ca-f803793ebf90"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Reading package lists...\n",
            "Building dependency tree...\n",
            "Reading state information...\n",
            "Package 'clojure' is not installed, so not removed\n",
            "0 upgraded, 0 newly installed, 0 to remove and 34 not upgraded.\n",
            "Downloading and expanding tar\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
            "100 14.4M  100 14.4M    0     0  27.8M      0 --:--:-- --:--:-- --:--:-- 70.7M\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Installing libs into /usr/local/lib/clojure\n",
            "Installing clojure and clj into /usr/local/bin\n",
            "Installing man pages into /usr/local/share/man/man1\n",
            "Removing download\n",
            "Use clj -h for help.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Cloning into 'daphne'...\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "Process(`\u001b[4mbash\u001b[24m \u001b[4m-lc\u001b[24m \u001b[4m'git clone https://github.com/plai-group/daphne.git'\u001b[24m`, ProcessExited(0))"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "run(`bash -lc \"sudo apt-get remove -y clojure\"`)\n",
        "run(`bash -lc \"curl -fsSL https://download.clojure.org/install/linux-install.sh | sudo bash\"`)\n",
        "run(`bash -lc \"git clone https://github.com/plai-group/daphne.git\"`)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "kXj9zXoNroMX"
      },
      "outputs": [],
      "source": [
        "using JSON\n",
        "using Main.EvaluationBasedSampling: AbstractSyntaxTree, evaluate_program\n",
        "using Main.GeneralSampling: get_sample, calculate_effective_sample_size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nN179h6WLaGo",
        "outputId": "08259868-362a-4fa3-91ae-54d781462ea9"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING: abs already refers to: #'clojure.core/abs in namespace: anglican.runtime, being replaced by: #'anglican.runtime/abs\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Expression (before): Any[\"let\", Any[\"mu\", Any[\"sample\", Any[\"normal\", 1, Any[\"sqrt\", 5]]]], Any[\"let\", Any[\"sigma\", Any[\"sqrt\", 2]], Any[\"let\", Any[\"lik\", Any[\"normal\", \"mu\", \"sigma\"]], Any[\"let\", Any[\"dontcare0\", Any[\"observe\", \"lik\", 8]], Any[\"let\", Any[\"dontcare1\", Any[\"observe\", \"lik\", 9]], \"mu\"]]]]]\n",
            "Expression (after): Any[\"let\", Any[\"mu\", Any[\"sample\", Any[\"normal\", 1, Any[\"sqrt\", 5]]]], Any[\"let\", Any[\"sigma\", Any[\"sqrt\", 2]], Any[\"let\", Any[\"lik\", Any[\"normal\", \"mu\", \"sigma\"]], Any[\"let\", Any[\"dontcare0\", Any[\"observe\", \"lik\", 8]], Any[\"let\", Any[\"dontcare1\", Any[\"observe\", \"lik\", 9]], \"mu\"]]]]]\n",
            "Result: 0.6023801023788922Float64\n",
            "\n",
            "→ Single draw μ = 0.6023801023788922\n",
            "→ log-joint = -35.58169191833986\n",
            "\n",
            "Drew 10000 samples; ESS = 88.11749\n"
          ]
        }
      ],
      "source": [
        "model1 = \"\"\"\n",
        "(let [mu    (sample (normal 1 (sqrt 5)))\n",
        "      sigma (sqrt 2)\n",
        "      lik   (normal mu sigma)]\n",
        "  (observe lik 8)\n",
        "  (observe lik 9)\n",
        "  mu)\n",
        "\"\"\"\n",
        "write(\"daphne/model1.daphne\", model1)\n",
        "\n",
        "cd(\"daphne\") do\n",
        "  run(`bash -lc \"clojure -M:run desugar  -i model1.daphne  -o model1_ast.json\"`)\n",
        "end\n",
        "\n",
        "ast = AbstractSyntaxTree(JSON.parsefile(\"daphne/model1_ast.json\"))\n",
        "μ, sig, _ = evaluate_program(ast; verbose=true)\n",
        "println(\"\\n→ Single draw μ = \", μ)\n",
        "println(\"→ log-joint = \", sig[\"logP\"])\n",
        "\n",
        "N = 10000\n",
        "samples = Vector{Float64}(undef, N)\n",
        "logws   = Vector{Float64}(undef, N)\n",
        "for i in 1:N\n",
        "    s, sg = get_sample(ast, \"desugar\"; verbose=false)\n",
        "    samples[i] = s\n",
        "    logws[i]   = sg[\"logW\"]\n",
        "end\n",
        "\n",
        "ws = exp.(logws .- maximum(logws))\n",
        "ess = calculate_effective_sample_size(ws)\n",
        "println(\"\\nDrew $N samples; ESS = \", round(ess, digits=5))  # Extremely inefficient and no convergence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5o10ehx-fXhf",
        "outputId": "885ee2a7-0a55-4b06-cf90-08ae1f91c29b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING: abs already refers to: #'clojure.core/abs in namespace: anglican.runtime, being replaced by: #'anglican.runtime/abs\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Expression (before): Any[\"let\", Any[\"slope\", Any[\"sample\", Any[\"normal\", 0.0, 10.0]]], Any[\"let\", Any[\"bias\", Any[\"sample\", Any[\"normal\", 0.0, 10.0]]], Any[\"let\", Any[\"data\", Any[\"vector\", 1.0, 2.1, 2.0, 3.9, 3.0, 5.3, 4.0, 7.7, 5.0, 10.2, 6.0, 12.9]], Any[\"let\", Any[\"dontcare1\", Any[\"let\", Any[\"a2\", \"slope\"], Any[\"let\", Any[\"a3\", \"bias\"], Any[\"let\", Any[\"acc4\", Any[\"observe-data\", 0, \"data\", \"a2\", \"a3\"]], Any[\"let\", Any[\"acc5\", Any[\"observe-data\", 1, \"acc4\", \"a2\", \"a3\"]], Any[\"let\", Any[\"acc6\", Any[\"observe-data\", 2, \"acc5\", \"a2\", \"a3\"]], Any[\"let\", Any[\"acc7\", Any[\"observe-data\", 3, \"acc6\", \"a2\", \"a3\"]], Any[\"let\", Any[\"acc8\", Any[\"observe-data\", 4, \"acc7\", \"a2\", \"a3\"]], Any[\"let\", Any[\"acc9\", Any[\"observe-data\", 5, \"acc8\", \"a2\", \"a3\"]], \"acc9\"]]]]]]]]], Any[\"vector\", \"slope\", \"bias\"]]]]]\n",
            "Expression (after): Any[\"let\", Any[\"slope\", Any[\"sample\", Any[\"normal\", 0.0, 10.0]]], Any[\"let\", Any[\"bias\", Any[\"sample\", Any[\"normal\", 0.0, 10.0]]], Any[\"let\", Any[\"data\", Any[\"vector\", 1.0, 2.1, 2.0, 3.9, 3.0, 5.3, 4.0, 7.7, 5.0, 10.2, 6.0, 12.9]], Any[\"let\", Any[\"dontcare1\", Any[\"let\", Any[\"a2\", \"slope\"], Any[\"let\", Any[\"a3\", \"bias\"], Any[\"let\", Any[\"acc4\", Any[\"observe-data\", 0, \"data\", \"a2\", \"a3\"]], Any[\"let\", Any[\"acc5\", Any[\"observe-data\", 1, \"acc4\", \"a2\", \"a3\"]], Any[\"let\", Any[\"acc6\", Any[\"observe-data\", 2, \"acc5\", \"a2\", \"a3\"]], Any[\"let\", Any[\"acc7\", Any[\"observe-data\", 3, \"acc6\", \"a2\", \"a3\"]], Any[\"let\", Any[\"acc8\", Any[\"observe-data\", 4, \"acc7\", \"a2\", \"a3\"]], Any[\"let\", Any[\"acc9\", Any[\"observe-data\", 5, \"acc8\", \"a2\", \"a3\"]], \"acc9\"]]]]]]]]], Any[\"vector\", \"slope\", \"bias\"]]]]]\n",
            "Result: [8.170192758822942, -8.537252318699634]Vector{Float64}\n",
            "\n",
            "→ Sampled [slope, bias] = [8.170192758822942, -8.537252318699634]\n",
            "→ log-joint = -840.4138793089326\n"
          ]
        }
      ],
      "source": [
        "model2 = \"\"\"\n",
        "(defn observe-data [_ data slope bias]\n",
        "  (let [xn (first data)\n",
        "        yn (second data)\n",
        "        zn (+ (* slope xn) bias)]\n",
        "    (observe (normal zn 1.0) yn)\n",
        "    (rest (rest data))))\n",
        "(let [slope (sample (normal 0.0 10.0))\n",
        "      bias  (sample (normal 0.0 10.0))\n",
        "      data  (vector 1.0 2.1 2.0 3.9 3.0 5.3\n",
        "                   4.0 7.7 5.0 10.2 6.0 12.9)]\n",
        "  (loop 6 data observe-data slope bias)\n",
        "  (vector slope bias))\n",
        "\"\"\"\n",
        "write(\"daphne/model2.daphne\", model2)\n",
        "\n",
        "cd(\"daphne\") do\n",
        "  run(`bash -lc \"clojure -M:run desugar  -i model2.daphne  -o model2_ast.json\"`)\n",
        "end\n",
        "\n",
        "ast1 = AbstractSyntaxTree(JSON.parsefile(\"daphne/model2_ast.json\"))\n",
        "params1, sig1, _ = evaluate_program(ast1; verbose=true)\n",
        "println(\"\\n→ Sampled [slope, bias] = \", params1)\n",
        "println(\"→ log-joint = \", sig1[\"logP\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QOmfJpSPfYGI",
        "outputId": "93a2fd2b-074e-478f-b386-3ceba3d475d4"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING: abs already refers to: #'clojure.core/abs in namespace: anglican.runtime, being replaced by: #'anglican.runtime/abs\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "#11 (generic function with 1 method)"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model3 = \"\"\"\n",
        "(defn hmm-step [t states data trans-dists likes]\n",
        "  (let [z (sample (get trans-dists (last states)))]\n",
        "    (observe (get likes z) (get data t))\n",
        "    (append states z)))\n",
        "(let [data        [0.9 0.8 0.7 0.0 -0.025 -5.0 -2.0 -0.1\n",
        "                   0.0 0.13 0.45 6.0 0.2 0.3 -1.0 -1.0]\n",
        "      trans-dists [(discrete [0.10 0.50 0.40])\n",
        "                   (discrete [0.20 0.20 0.60])\n",
        "                   (discrete [0.15 0.15 0.70])]\n",
        "      likes       [(normal -1.0 1.0)\n",
        "                   (normal  1.0 1.0)\n",
        "                   (normal  0.0 1.0)]\n",
        "      states      [(sample (discrete [0.33 0.33 0.34]))]]\n",
        "  (loop 16 states hmm-step data trans-dists likes))\n",
        "\"\"\"\n",
        "\n",
        "write(\"daphne/model3.daphne\", model3)\n",
        "\n",
        "cd(\"daphne\") do\n",
        "  run(`bash -lc \"clojure -M:run desugar  -i model3.daphne  -o model3_ast.json\"`)\n",
        "end\n",
        "\n",
        "\n",
        "\n",
        "# TODO: Update the following methods inside the primitives module itself\n",
        "import Main.CustomCategorical: CategoricalDist as _origCatDist\n",
        "\n",
        "function CategoricalDist(probs::AbstractVector; validate_args::Bool=false)\n",
        "    return _origCatDist(probs)\n",
        "end\n",
        "\n",
        "Main.BasicPrimitives.primitives[\"discrete\"] =\n",
        "    (p; kwargs...) -> Main.BasicPrimitives.zero_based_categorical(p)\n",
        "\n",
        "Main.BasicPrimitives.primitives[\"discrete-guide\"] =\n",
        "    (p; kwargs...) -> Main.BasicPrimitives.zero_based_categorical(p)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4gH4hxXLzCw6",
        "outputId": "d8e99118-bfac-4e6a-f650-4e7dc96f93b0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Expression (before): Any[\"let\", Any[\"data\", Any[\"vector\", 0.9, 0.8, 0.7, 0.0, -0.025, -5.0, -2.0, -0.1, 0.0, 0.13, 0.45, 6.0, 0.2, 0.3, -1.0, -1.0]], Any[\"let\", Any[\"trans-dists\", Any[\"vector\", Any[\"discrete\", Any[\"vector\", 0.1, 0.5, 0.4]], Any[\"discrete\", Any[\"vector\", 0.2, 0.2, 0.6]], Any[\"discrete\", Any[\"vector\", 0.15, 0.15, 0.7]]]], Any[\"let\", Any[\"likes\", Any[\"vector\", Any[\"normal\", -1.0, 1.0], Any[\"normal\", 1.0, 1.0], Any[\"normal\", 0.0, 1.0]]], Any[\"let\", Any[\"states\", Any[\"vector\", Any[\"sample\", Any[\"discrete\", Any[\"vector\", 0.33, 0.33, 0.34]]]]], Any[\"let\", Any[\"a1\", \"data\"], Any[\"let\", Any[\"a2\", \"trans-dists\"], Any[\"let\", Any[\"a3\", \"likes\"], Any[\"let\", Any[\"acc4\", Any[\"hmm-step\", 0, \"states\", \"a1\", \"a2\", \"a3\"]], Any[\"let\", Any[\"acc5\", Any[\"hmm-step\", 1, \"acc4\", \"a1\", \"a2\", \"a3\"]], Any[\"let\", Any[\"acc6\", Any[\"hmm-step\", 2, \"acc5\", \"a1\", \"a2\", \"a3\"]], Any[\"let\", Any[\"acc7\", Any[\"hmm-step\", 3, \"acc6\", \"a1\", \"a2\", \"a3\"]], Any[\"let\", Any[\"acc8\", Any[\"hmm-step\", 4, \"acc7\", \"a1\", \"a2\", \"a3\"]], Any[\"let\", Any[\"acc9\", Any[\"hmm-step\", 5, \"acc8\", \"a1\", \"a2\", \"a3\"]], Any[\"let\", Any[\"acc10\", Any[\"hmm-step\", 6, \"acc9\", \"a1\", \"a2\", \"a3\"]], Any[\"let\", Any[\"acc11\", Any[\"hmm-step\", 7, \"acc10\", \"a1\", \"a2\", \"a3\"]], Any[\"let\", Any[\"acc12\", Any[\"hmm-step\", 8, \"acc11\", \"a1\", \"a2\", \"a3\"]], Any[\"let\", Any[\"acc13\", Any[\"hmm-step\", 9, \"acc12\", \"a1\", \"a2\", \"a3\"]], Any[\"let\", Any[\"acc14\", Any[\"hmm-step\", 10, \"acc13\", \"a1\", \"a2\", \"a3\"]], Any[\"let\", Any[\"acc15\", Any[\"hmm-step\", 11, \"acc14\", \"a1\", \"a2\", \"a3\"]], Any[\"let\", Any[\"acc16\", Any[\"hmm-step\", 12, \"acc15\", \"a1\", \"a2\", \"a3\"]], Any[\"let\", Any[\"acc17\", Any[\"hmm-step\", 13, \"acc16\", \"a1\", \"a2\", \"a3\"]], Any[\"let\", Any[\"acc18\", Any[\"hmm-step\", 14, \"acc17\", \"a1\", \"a2\", \"a3\"]], Any[\"let\", Any[\"acc19\", Any[\"hmm-step\", 15, \"acc18\", \"a1\", \"a2\", \"a3\"]], \"acc19\"]]]]]]]]]]]]]]]]]]]]]]]\n",
            "Expression (after): Any[\"let\", Any[\"data\", Any[\"vector\", 0.9, 0.8, 0.7, 0.0, -0.025, -5.0, -2.0, -0.1, 0.0, 0.13, 0.45, 6.0, 0.2, 0.3, -1.0, -1.0]], Any[\"let\", Any[\"trans-dists\", Any[\"vector\", Any[\"discrete\", Any[\"vector\", 0.1, 0.5, 0.4]], Any[\"discrete\", Any[\"vector\", 0.2, 0.2, 0.6]], Any[\"discrete\", Any[\"vector\", 0.15, 0.15, 0.7]]]], Any[\"let\", Any[\"likes\", Any[\"vector\", Any[\"normal\", -1.0, 1.0], Any[\"normal\", 1.0, 1.0], Any[\"normal\", 0.0, 1.0]]], Any[\"let\", Any[\"states\", Any[\"vector\", Any[\"sample\", Any[\"discrete\", Any[\"vector\", 0.33, 0.33, 0.34]]]]], Any[\"let\", Any[\"a1\", \"data\"], Any[\"let\", Any[\"a2\", \"trans-dists\"], Any[\"let\", Any[\"a3\", \"likes\"], Any[\"let\", Any[\"acc4\", Any[\"hmm-step\", 0, \"states\", \"a1\", \"a2\", \"a3\"]], Any[\"let\", Any[\"acc5\", Any[\"hmm-step\", 1, \"acc4\", \"a1\", \"a2\", \"a3\"]], Any[\"let\", Any[\"acc6\", Any[\"hmm-step\", 2, \"acc5\", \"a1\", \"a2\", \"a3\"]], Any[\"let\", Any[\"acc7\", Any[\"hmm-step\", 3, \"acc6\", \"a1\", \"a2\", \"a3\"]], Any[\"let\", Any[\"acc8\", Any[\"hmm-step\", 4, \"acc7\", \"a1\", \"a2\", \"a3\"]], Any[\"let\", Any[\"acc9\", Any[\"hmm-step\", 5, \"acc8\", \"a1\", \"a2\", \"a3\"]], Any[\"let\", Any[\"acc10\", Any[\"hmm-step\", 6, \"acc9\", \"a1\", \"a2\", \"a3\"]], Any[\"let\", Any[\"acc11\", Any[\"hmm-step\", 7, \"acc10\", \"a1\", \"a2\", \"a3\"]], Any[\"let\", Any[\"acc12\", Any[\"hmm-step\", 8, \"acc11\", \"a1\", \"a2\", \"a3\"]], Any[\"let\", Any[\"acc13\", Any[\"hmm-step\", 9, \"acc12\", \"a1\", \"a2\", \"a3\"]], Any[\"let\", Any[\"acc14\", Any[\"hmm-step\", 10, \"acc13\", \"a1\", \"a2\", \"a3\"]], Any[\"let\", Any[\"acc15\", Any[\"hmm-step\", 11, \"acc14\", \"a1\", \"a2\", \"a3\"]], Any[\"let\", Any[\"acc16\", Any[\"hmm-step\", 12, \"acc15\", \"a1\", \"a2\", \"a3\"]], Any[\"let\", Any[\"acc17\", Any[\"hmm-step\", 13, \"acc16\", \"a1\", \"a2\", \"a3\"]], Any[\"let\", Any[\"acc18\", Any[\"hmm-step\", 14, \"acc17\", \"a1\", \"a2\", \"a3\"]], Any[\"let\", Any[\"acc19\", Any[\"hmm-step\", 15, \"acc18\", \"a1\", \"a2\", \"a3\"]], \"acc19\"]]]]]]]]]]]]]]]]]]]]]]]\n",
            "Result: [1, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 1, 2, 1]Vector{Int64}\n",
            "\n",
            "Result                  = [1, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 1, 2, 1]\n",
            "Log-joint probability   = -67.365971105851\n"
          ]
        }
      ],
      "source": [
        "using JSON\n",
        "ast_json = JSON.parsefile(\"daphne/model3_ast.json\")\n",
        "ast      = Main.EvaluationBasedSampling.AbstractSyntaxTree(ast_json)\n",
        "\n",
        "result, sig, l = Main.EvaluationBasedSampling.evaluate_program(ast; verbose=true)\n",
        "\n",
        "println(\"\\nResult                  = \", result)\n",
        "println(\"Log-joint probability   = \", sig[\"logP\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ie73No5NAcNR",
        "outputId": "c23d4eef-3f1d-4b5b-ad30-06628d0f1d89"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "#14 (generic function with 1 method)"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import Main.BasicPrimitives\n",
        "import Distributions\n",
        "\n",
        "# TODO: Update the following methods inside the primitives module itself\n",
        "\n",
        "BasicPrimitives.primitives[\"normal\"] =\n",
        "    (μ, σ; kwargs...) -> Distributions.Normal(float(μ), float(σ))\n",
        "\n",
        "BasicPrimitives.primitives[\"normal-guide\"] =\n",
        "    BasicPrimitives.primitives[\"normal\"]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9uZrys0WkIsD"
      },
      "outputs": [],
      "source": [
        "model4 = \"\"\"\n",
        "(let [weight-prior (normal 0 1)\n",
        "      W_0 (foreach 10 [] (foreach 1 [] (sample weight-prior)))\n",
        "      W_1 (foreach 10 [] (foreach 10 [] (sample weight-prior)))\n",
        "      W_2 (foreach 1 [] (foreach 10 [] (sample weight-prior)))\n",
        "\n",
        "      b_0 (foreach 10 [] (foreach 1 [] (sample weight-prior)))\n",
        "      b_1 (foreach 10 [] (foreach 1 [] (sample weight-prior)))\n",
        "      b_2 (foreach 1 [] (foreach 1 [] (sample weight-prior)))\n",
        "\n",
        "      x   (mat-transpose [[1] [2] [3] [4] [5]])\n",
        "      y   [[1] [4] [9] [16] [25]]\n",
        "      h_0 (mat-tanh (mat-add (mat-mul W_0 x)\n",
        "                             (mat-repmat b_0 1 5)))\n",
        "      h_1 (mat-tanh (mat-add (mat-mul W_1 h_0)\n",
        "                             (mat-repmat b_1 1 5)))\n",
        "      mu  (mat-transpose\n",
        "            (mat-tanh (mat-add (mat-mul W_2 h_1)\n",
        "                               (mat-repmat b_2 1 5))))]\n",
        "  (foreach 5 [y_r y\n",
        "              mu_r mu]\n",
        "    (foreach 1 [y_rc y_r\n",
        "                mu_rc mu_r]\n",
        "      (observe (normal mu_rc 1) y_rc)))\n",
        "  [W_0 b_0 W_1 b_1])\n",
        "\"\"\"\n",
        "write(\"daphne/model4.daphne\", model4)\n",
        "\n",
        "cd(\"daphne\") do\n",
        "  run(`bash -lc \"clojure -M:run desugar -i model4.daphne -o model4_ast.json\"`)\n",
        "end\n",
        "\n",
        "ast3 = AbstractSyntaxTree(JSON.parsefile(\"daphne/model4_ast.json\"))\n",
        "params3, sig3, _ = evaluate_program(ast3; verbose=true)\n",
        "\n",
        "println(\"\\n→ Sampled [W_0, b_0, W_1, b_1] shapes/types = \", params3)\n",
        "println(\"→ log-joint = \", sig3[\"logP\"])"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "JWwEtIHCK_F4",
        "wsh-7i5eAhRm",
        "E0ojCsPUADaX",
        "-Qh0TnQyAunu",
        "U5mM2wJEAE-c"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Julia",
      "name": "julia"
    },
    "language_info": {
      "name": "julia"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
